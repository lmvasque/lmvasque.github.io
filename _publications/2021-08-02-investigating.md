---
title: "Investigating Text Simplification Evaluation"
collection: publications
category: conferences
permalink: /publication/2021-08-02-investigating
excerpt: 'Modern text simplification (TS) heavily relies on the availability of gold standard data to build machine learning models. However, existing studies show that parallel TS corpora contain inaccurate simplifications and incorrect alignments. Additionally, evaluation is usually performed by using metrics such as BLEU or SARI to compare system output to the gold standard. A major limitation is that these metrics do not match human judgements and the performance on different datasets and linguistic phenomena vary greatly. Furthermore, our research shows that the test and training subsets of parallel datasets differ significantly. In this work, we investigate existing TS corpora, providing new insights that will motivate the improvement of existing state-of-the-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models.'
date: 2021-08-02
venue: 'Evaluation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. (Online).'
slidesurl: 'https://aclanthology.org/2021.findings-acl.77.mp4'
paperurl: 'https://aclanthology.org/2021.findings-acl.77/'
citation: 'Vásquez-Rodríguez, L., Shardlow, M., Przybyła, P., and Ananiadou, S. (2021). Investigating Text Simplification Evaluation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. (Online).'
---
Modern text simplification (TS) heavily relies on the availability of gold standard data to build machine learning models. However, existing studies show that parallel TS corpora contain inaccurate simplifications and incorrect alignments. Additionally, evaluation is usually performed by using metrics such as BLEU or SARI to compare system output to the gold standard. A major limitation is that these metrics do not match human judgements and the performance on different datasets and linguistic phenomena vary greatly. Furthermore, our research shows that the test and training subsets of parallel datasets differ significantly. In this work, we investigate existing TS corpora, providing new insights that will motivate the improvement of existing state-of-the-art TS evaluation methods. Our contributions include the analysis of TS corpora based on existing modifications used for simplification and an empirical study on TS models performance by using better-distributed datasets. We demonstrate that by improving the distribution of TS datasets, we can build more robust TS models.

[Download slides here](https://aclanthology.org/2021.findings-acl.77.mp4)

[Download paper here](https://aclanthology.org/2021.findings-acl.77/)

Recommended citation: Vásquez-Rodríguez, L., Shardlow, M., Przybyła, P., and Ananiadou, S. (2021). Investigating Text Simplification Evaluation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. (Online).